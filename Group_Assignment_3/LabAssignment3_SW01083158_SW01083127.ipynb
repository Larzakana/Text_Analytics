{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae0fecb-f75d-4088-8b32-d7ccb9d71bf5",
   "metadata": {},
   "source": [
    "# LAB ASSIGNMENT 3\n",
    "## TEXT ANALYTICS\n",
    "### NAME:\n",
    "- AIMI AFIFAH BINTI MUHAMMAD HUSNI (SW01083127)\n",
    "- SARAH HANI BINTI MOHD AZRAL (SW01083158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9028ae5d-ec6c-4494-ad29-e0df2dd642b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b3459b8-c1e9-4425-9890-3bef9d8e202f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99ce112-d158-4606-90ba-38c09cf30b5d",
   "metadata": {},
   "source": [
    "- This code imports necessary libraries: pandas, re, nltk, gensim. It dDownloads NLTK resources for tokenization, stopwords, and WordNet It also p\n",
    "Provides essential functionalities for text preprocessing and topic modelinta.on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85df9e6f-2790-4a54-982d-48e78dfea81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I was wondering if anyone out there could enli...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>I recently posted an article asking what kind ...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>\\nIt depends on your priorities.  A lot of peo...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>an excellent automatic can be found in the sub...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>: Ford and his automobile.  I need information...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  target  \\\n",
       "0           0  I was wondering if anyone out there could enli...       7   \n",
       "1          17  I recently posted an article asking what kind ...       7   \n",
       "2          29  \\nIt depends on your priorities.  A lot of peo...       7   \n",
       "3          56  an excellent automatic can be found in the sub...       7   \n",
       "4          64  : Ford and his automobile.  I need information...       7   \n",
       "\n",
       "       title                        date  \n",
       "0  rec.autos  2022-08-02 13:48:37.251043  \n",
       "1  rec.autos  2022-08-02 13:48:37.251043  \n",
       "2  rec.autos  2022-08-02 13:48:37.251043  \n",
       "3  rec.autos  2022-08-02 13:48:37.251043  \n",
       "4  rec.autos  2022-08-02 13:48:37.251043  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data\n",
    "news_data = pd.read_csv(\"news_dataset.csv\")\n",
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6145b3ff-cb87-4203-9715-bb6fd466141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values and duplicates\n",
    "news_data = news_data.dropna(subset=['text'])\n",
    "news_data = news_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0d745ac-e695-4e54-81ac-54a476952a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11096, 5)\n",
      "    Unnamed: 0                                               text  target  \\\n",
      "0            0  I was wondering if anyone out there could enli...       7   \n",
      "1           17  I recently posted an article asking what kind ...       7   \n",
      "2           29  \\nIt depends on your priorities.  A lot of peo...       7   \n",
      "3           56  an excellent automatic can be found in the sub...       7   \n",
      "4           64  : Ford and his automobile.  I need information...       7   \n",
      "5           71  \\nYo! Watch the attributions--I didn't say tha...       7   \n",
      "6           73  \\nYou can avoid these problems entirely by ins...       7   \n",
      "7           77  I have a 1986 Acura Integra 5 speed with 95,00...       7   \n",
      "8           84  \\nassuming yours is a non turbo MR2, the gruff...       7   \n",
      "9          156  \\n\\nIn addition to restricted mileage, many cl...       7   \n",
      "10         201  \\n\\nSeveral years ago GM was having trouble wi...       7   \n",
      "11         207  \\nI had the same problem in my '90 MX-6. Lucki...       7   \n",
      "12         216  As an additional data point, I have run Castro...       7   \n",
      "13         262  \\nWomen's pants rarely have pockets and most, ...       7   \n",
      "14         274  \\n\\nI agree.  Six hour long stretches behind t...       7   \n",
      "15         405  From: thwang@mentor.cc.purdue.edu (Tommy Hwang...       7   \n",
      "16         415              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^...       7   \n",
      "17         445  \\n\\n\\n\\n\\n\\nBzzt.\\nThe manta was a two-door se...       7   \n",
      "18         462  sorry about that last post, my server neglecte...       7   \n",
      "19         463  From article <1993Apr5.200048.23421@ucsu.Color...       7   \n",
      "\n",
      "        title                        date  \n",
      "0   rec.autos  2022-08-02 13:48:37.251043  \n",
      "1   rec.autos  2022-08-02 13:48:37.251043  \n",
      "2   rec.autos  2022-08-02 13:48:37.251043  \n",
      "3   rec.autos  2022-08-02 13:48:37.251043  \n",
      "4   rec.autos  2022-08-02 13:48:37.251043  \n",
      "5   rec.autos  2022-08-02 13:48:37.251043  \n",
      "6   rec.autos  2022-08-02 13:48:37.251043  \n",
      "7   rec.autos  2022-08-02 13:48:37.251043  \n",
      "8   rec.autos  2022-08-02 13:48:37.251043  \n",
      "9   rec.autos  2022-08-02 13:48:37.251043  \n",
      "10  rec.autos  2022-08-02 13:48:37.251043  \n",
      "11  rec.autos  2022-08-02 13:48:37.251043  \n",
      "12  rec.autos  2022-08-02 13:48:37.251043  \n",
      "13  rec.autos  2022-08-02 13:48:37.251043  \n",
      "14  rec.autos  2022-08-02 13:48:37.251043  \n",
      "15  rec.autos  2022-08-02 13:48:37.251043  \n",
      "16  rec.autos  2022-08-02 13:48:37.251043  \n",
      "17  rec.autos  2022-08-02 13:48:37.251043  \n",
      "18  rec.autos  2022-08-02 13:48:37.251043  \n",
      "19  rec.autos  2022-08-02 13:48:37.251043  \n"
     ]
    }
   ],
   "source": [
    "# Check the shape and the first few rows of the data\n",
    "print(news_data.shape)\n",
    "print(news_data.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72936d9c-d4b6-48b1-a386-cca5b92752d3",
   "metadata": {},
   "source": [
    "- The code reads the dataset from a CSV file named \"news_dataset.csv\" using pandas' read_csv() function. \n",
    "It removes null values and duplicates from the 'text' column of the dataset using the dropna() and drop_duplicates() methods, respectively \n",
    "After cleaning the data, it prints the shape of the cleaned dataset to display the number of rows and column .\n",
    "Additionally, it displays the first 20 rows of the cleaned dataset to examine its structure and content. This helps in understanding the data before proceeding with further analysis or preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "808a3345-84b9-4548-b8be-d7ec9e0619c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 'text' column\n",
    "text_data = news_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5555913-42d9-4265-9f2d-4699d2051bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicated text\n",
    "text_duplicates = text_data.duplicated()\n",
    "print(text_duplicates.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e226a06-c1ff-4196-8b24-13235f35da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords and initialize stemmer and lemmatizer\n",
    "stop_words = stopwords.words('english')\n",
    "more_stopwords = ['u', 'im', 'c', 'maxaxaxaxaxaxaxaxaxaxaxaxaxaxax']\n",
    "stop_words.extend(more_stopwords)\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b701c19e-5b7c-4332-b9af-1b640205a77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)  # Remove words containing numbers\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)  # Remove emails\n",
    "    tokens = word_tokenize(text)  # Tokenize text\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c9a275-d6cb-486c-9085-2083b40ee448",
   "metadata": {},
   "source": [
    "- This code snippet sets up the initial steps for text preprocessing. It selects the 'text' column from the dataset and checks for duplicated text. Then, it defines stopwords, initializes a stemmer and a lemmatizer, and creates a preprocessing function. The function converts text to lowercase, removes punctuation, numbers, emails, and stopwords, and performs lemmatization. These steps lay the foundation for cleaning and preparing the text data for further analysis, such as topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d565c798-54f6-44fb-bb32-8b04a1090c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0                                               text  target  \\\n",
      "0            0  I was wondering if anyone out there could enli...       7   \n",
      "1           17  I recently posted an article asking what kind ...       7   \n",
      "2           29  \\nIt depends on your priorities.  A lot of peo...       7   \n",
      "3           56  an excellent automatic can be found in the sub...       7   \n",
      "4           64  : Ford and his automobile.  I need information...       7   \n",
      "5           71  \\nYo! Watch the attributions--I didn't say tha...       7   \n",
      "6           73  \\nYou can avoid these problems entirely by ins...       7   \n",
      "7           77  I have a 1986 Acura Integra 5 speed with 95,00...       7   \n",
      "8           84  \\nassuming yours is a non turbo MR2, the gruff...       7   \n",
      "9          156  \\n\\nIn addition to restricted mileage, many cl...       7   \n",
      "10         201  \\n\\nSeveral years ago GM was having trouble wi...       7   \n",
      "11         207  \\nI had the same problem in my '90 MX-6. Lucki...       7   \n",
      "12         216  As an additional data point, I have run Castro...       7   \n",
      "13         262  \\nWomen's pants rarely have pockets and most, ...       7   \n",
      "14         274  \\n\\nI agree.  Six hour long stretches behind t...       7   \n",
      "15         405  From: thwang@mentor.cc.purdue.edu (Tommy Hwang...       7   \n",
      "16         415              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^...       7   \n",
      "17         445  \\n\\n\\n\\n\\n\\nBzzt.\\nThe manta was a two-door se...       7   \n",
      "18         462  sorry about that last post, my server neglecte...       7   \n",
      "19         463  From article <1993Apr5.200048.23421@ucsu.Color...       7   \n",
      "\n",
      "        title                        date  \\\n",
      "0   rec.autos  2022-08-02 13:48:37.251043   \n",
      "1   rec.autos  2022-08-02 13:48:37.251043   \n",
      "2   rec.autos  2022-08-02 13:48:37.251043   \n",
      "3   rec.autos  2022-08-02 13:48:37.251043   \n",
      "4   rec.autos  2022-08-02 13:48:37.251043   \n",
      "5   rec.autos  2022-08-02 13:48:37.251043   \n",
      "6   rec.autos  2022-08-02 13:48:37.251043   \n",
      "7   rec.autos  2022-08-02 13:48:37.251043   \n",
      "8   rec.autos  2022-08-02 13:48:37.251043   \n",
      "9   rec.autos  2022-08-02 13:48:37.251043   \n",
      "10  rec.autos  2022-08-02 13:48:37.251043   \n",
      "11  rec.autos  2022-08-02 13:48:37.251043   \n",
      "12  rec.autos  2022-08-02 13:48:37.251043   \n",
      "13  rec.autos  2022-08-02 13:48:37.251043   \n",
      "14  rec.autos  2022-08-02 13:48:37.251043   \n",
      "15  rec.autos  2022-08-02 13:48:37.251043   \n",
      "16  rec.autos  2022-08-02 13:48:37.251043   \n",
      "17  rec.autos  2022-08-02 13:48:37.251043   \n",
      "18  rec.autos  2022-08-02 13:48:37.251043   \n",
      "19  rec.autos  2022-08-02 13:48:37.251043   \n",
      "\n",
      "                                           clean_text  \n",
      "0   [wondering, anyone, could, enlighten, car, saw...  \n",
      "1   [recently, posted, article, asking, kind, rate...  \n",
      "2   [depends, priority, lot, people, put, higher, ...  \n",
      "3   [excellent, automatic, found, subaru, legacy, ...  \n",
      "4   [ford, automobile, need, information, whether,...  \n",
      "5   [yo, watch, attributionsi, didnt, say, isnt, a...  \n",
      "6   [avoid, problem, entirely, installing, oil, dr...  \n",
      "7   [acura, integra, speed, mile, positively, wors...  \n",
      "8   [assuming, non, turbo, gruffness, characterist...  \n",
      "9   [addition, restricted, mileage, many, classic,...  \n",
      "10  [several, year, ago, gm, trouble, ring, sticki...  \n",
      "11  [problem, luckily, fixed, warranty, think, rep...  \n",
      "12  [additional, data, point, run, castrol, exclus...  \n",
      "13  [woman, pant, rarely, pocket, shallow, use, im...  \n",
      "14  [agree, six, hour, long, stretch, behind, whee...  \n",
      "15  [thwangmentorccpurdueedu, tommy, hwang, subjec...  \n",
      "16  [know, isnt, group, since, brought, anyone, id...  \n",
      "17  [bzzt, manta, twodoor, sedan, u, engine, somet...  \n",
      "18  [sorry, last, post, server, neglected, send, m...  \n",
      "19  [article, lorenzorintintincoloradoedu, eric, l...  \n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing\n",
    "news_data['clean_text'] = news_data['text'].apply(preprocess_text)\n",
    "print(news_data.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ef9ab-e71f-4bb5-8630-8f0590618420",
   "metadata": {},
   "source": [
    "- This code applies the preprocessing steps to the 'text' column of the dataset. It utilizes the previously defined preprocess_text() function to convert the text into lowercase, remove punctuation, numbers, emails, tokenize the text, remove stopwords, and perform lemmatization. The preprocessed text is then stored in a new column named 'clean_text'. The print(news_data.head(20)) statement displays the first 20 rows of the dataset, including the original text and the corresponding preprocessed text. This allows for verification of the preprocessing steps and ensures that the data is ready for further analysis, such as topic modeling with LDA.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf07008e-1d10-4e17-82e5-54dfe4ef82af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gensim Dictionary object from the preprocessed documents\n",
    "dictionary = corpora.Dictionary(news_data['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "090436cc-c54d-4ad3-8c4c-d644ea400c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each preprocessed document into a bag-of-words representation using the dictionary\n",
    "corpus = [dictionary.doc2bow(doc) for doc in news_data['clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2fe3db9-4f4e-417e-b4ce-62a2271cd38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform LDA using Gensim library\n",
    "lda_model = LdaModel(corpus, num_topics=4, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6510fa2a-9310-46b9-ae48-1d054e6a338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store dominant topic labels for each document\n",
    "article_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36977bc4-76ae-455a-9c55-9d62aefe6648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each processed document\n",
    "for i, doc in enumerate(news_data['clean_text']):\n",
    "    # Convert each document to bag-of-words representation\n",
    "    bow = dictionary.doc2bow(doc)\n",
    "    # Get list of topic probabilities\n",
    "    topics = lda_model.get_document_topics(bow)\n",
    "    # Determine topic with highest probability\n",
    "    dominant_topic = max(topics, key=lambda x: x[1])[0]\n",
    "    # Append to the list\n",
    "    article_labels.append(dominant_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b00ae-1f45-4e42-bbba-c89fe2a14972",
   "metadata": {},
   "source": [
    "- This code applies Latent Dirichlet Allocation (LDA) using the Gensim library to the preprocessed text data. It first creates a dictionary mapping words to numerical IDs from the preprocessed documents. Then, it converts each document into a bag-of-words representation using this dictionary. Next, LDA is performed with a specified number of topics and passes over the corpus. Finally, it determines the dominant topic for each document and stores the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b69831c2-0eab-415c-9179-198bc966282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table with Articles and Topics:\n",
      "                                                    Text  Topic\n",
      "0      I was wondering if anyone out there could enli...      0\n",
      "1      I recently posted an article asking what kind ...      0\n",
      "2      \\nIt depends on your priorities.  A lot of peo...      0\n",
      "3      an excellent automatic can be found in the sub...      0\n",
      "4      : Ford and his automobile.  I need information...      0\n",
      "...                                                  ...    ...\n",
      "11309  Secrecy in Clipper Chip\\n\\nThe serial number o...      2\n",
      "11310  Hi !\\n\\nI am interested in the source of FEAL ...      2\n",
      "11311  The actual algorithm is classified, however, t...      2\n",
      "11312  \\n\\tThis appears to be generic calling upon th...      0\n",
      "11313  \\nProbably keep quiet and take it, lest they g...      0\n",
      "\n",
      "[11096 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe to see them\n",
    "topic_df = pd.DataFrame({\"Text\": news_data['text'], \"Topic\": article_labels})\n",
    "print(\"Table with Articles and Topics:\")\n",
    "print(topic_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f9ae22-aec8-4bb7-b547-f9235f923656",
   "metadata": {},
   "source": [
    "- This part of the code creates a new DataFrame named topic_df to display the original text along with the assigned topic labels. It constructs the DataFrame by combining the 'text' column from the original dataset with the 'article_labels' list generated during the topic modeling process. Each row in the DataFrame represents a piece of text along with its assigned topic label. Finally, it prints out the DataFrame to visually inspect the articles and their corresponding topics. This allows for a better understanding of how the topics are distributed across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d684e8e-6632-4b2f-9899-0f42f99cd68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "- \"would\" (weight: 0.008)\n",
      "- \"one\" (weight: 0.008)\n",
      "- \"people\" (weight: 0.007)\n",
      "- \"dont\" (weight: 0.005)\n",
      "- \"think\" (weight: 0.005)\n",
      "- \"know\" (weight: 0.005)\n",
      "- \"like\" (weight: 0.004)\n",
      "- \"time\" (weight: 0.004)\n",
      "- \"u\" (weight: 0.004)\n",
      "- \"say\" (weight: 0.004)\n",
      "\n",
      "Topic 1:\n",
      "- \"db\" (weight: 0.022)\n",
      "- \"space\" (weight: 0.006)\n",
      "- \"car\" (weight: 0.005)\n",
      "- \"mov\" (weight: 0.005)\n",
      "- \"power\" (weight: 0.005)\n",
      "- \"bike\" (weight: 0.004)\n",
      "- \"period\" (weight: 0.003)\n",
      "- \"one\" (weight: 0.003)\n",
      "- \"wire\" (weight: 0.003)\n",
      "- \"bit\" (weight: 0.003)\n",
      "\n",
      "Topic 2:\n",
      "- \"x\" (weight: 0.014)\n",
      "- \"key\" (weight: 0.009)\n",
      "- \"use\" (weight: 0.007)\n",
      "- \"system\" (weight: 0.007)\n",
      "- \"file\" (weight: 0.006)\n",
      "- \"chip\" (weight: 0.005)\n",
      "- \"one\" (weight: 0.005)\n",
      "- \"encryption\" (weight: 0.005)\n",
      "- \"get\" (weight: 0.004)\n",
      "- \"program\" (weight: 0.004)\n",
      "\n",
      "Topic 3:\n",
      "- \"game\" (weight: 0.010)\n",
      "- \"team\" (weight: 0.009)\n",
      "- \"year\" (weight: 0.005)\n",
      "- \"player\" (weight: 0.005)\n",
      "- \"new\" (weight: 0.005)\n",
      "- \"university\" (weight: 0.004)\n",
      "- \"season\" (weight: 0.004)\n",
      "- \"league\" (weight: 0.004)\n",
      "- \"play\" (weight: 0.003)\n",
      "- \"win\" (weight: 0.003)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print top terms for each topic\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"Topic {idx}:\")\n",
    "    terms = [term.strip() for term in topic.split(\"+\")]\n",
    "    for term in terms:\n",
    "        weight, word = term.split(\"*\")\n",
    "        print(f\"- {word.strip()} (weight: {weight.strip()})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bb9697-eef4-4515-9a6b-12c9ace3060c",
   "metadata": {},
   "source": [
    "- This section of the code prints the top terms for each topic identified by the LDA model. For each topic, it iterates through the list of terms and their corresponding weights, printing them out in a human-readable format. Each line displays a term along with its weight within the topic. These terms represent the most relevant words associated with each topic as identified by the LDA model. Understanding these top terms can provide insight into the themes or subjects covered by each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75e4fd07-a404-48d3-a0b3-2ccae83001c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms for Topic #0:\n",
      "['would', 'one', 'people', 'dont', 'think', 'know', 'like', 'time', 'u', 'say']\n",
      "\n",
      "Top terms for Topic #1:\n",
      "['db', 'space', 'car', 'mov', 'power', 'bike', 'period', 'one', 'wire', 'bit']\n",
      "\n",
      "Top terms for Topic #2:\n",
      "['x', 'key', 'use', 'system', 'file', 'chip', 'one', 'encryption', 'get', 'program']\n",
      "\n",
      "Top terms for Topic #3:\n",
      "['game', 'team', 'year', 'player', 'new', 'university', 'season', 'league', 'play', 'win']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print top terms for each topic\n",
    "for topic_id in range(lda_model.num_topics):\n",
    "    print(f\"Top terms for Topic #{topic_id}:\")\n",
    "    top_terms = lda_model.show_topic(topic_id, topn=10)\n",
    "    print([term[0] for term in top_terms])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fee1fe-45bf-4a23-8e2f-0b137522429e",
   "metadata": {},
   "source": [
    "- This code also prints the top terms for each topic identified by the LDA model. It iterates through each topic and utilizes the show_topic() method to retrieve the top N terms for that topic, where N is specified by the topn parameter. It then prints out these top terms for each topic, providing insight into the main themes or concepts associated with each topic. This representation can be helpful in interpreting and understanding the topics generated by the LDA model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
